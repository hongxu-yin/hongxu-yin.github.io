<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hongxu (Danny) Yin</title>

  <meta name="author" content="Hongxu (Danny) Yin">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hongxu (Danny) Yin</name>
              </p>
              <p>I am a senior research scientist at <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, with the learning and perception research team led by <a href="https://jankautz.com/">Jan Kautz</a>.
                I did my Ph.D. at <a href="https://ece.princeton.edu/">Princeton University</a>, where I was advised by Prof. <a href="https://ece.princeton.edu/people/niraj-jha">Niraj Jha</a>.
              </p>
                <p>I am a recipient of Princeton Yan Huo 94* Graduate Fellowship, Princeton ECE Best Dissertation Finalist, Princeton Natural Sciences and Engineering Fellowship, Defense Science & Technology Agency gold medal, and Thomson Asia Pacific Holdings gold medal.
                </p><p>I have also been featured by <a href="https://www.forbes.com/">Forbes</a> as Top 60 Elite Chinese in North America and  <a href="https://36kr.com/">36Kr</a> with Global Outstanding Chinese Power 100 Award.
              </p>
              <p style="text-align:center">
                <a href="mailto:dannyy@nvidia.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=4gdSoOYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/yin_hongxu">Twitter</a> /&nbsp
                <a href="data/hongxu_cv.pdf">CV</a> &nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/danny_yin_photo.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/danny_yin_photo.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in a deeper understanding of neural nets.
              </p>

              <p>
                This makes neural net efficient and secure. Topics cover data-free and efficient deep learning, overseeing CNNs and transformers, leveraging model
                inversion, knowledge distillation, dynamic inference, neural architecture search, pruning, quantization, model adaptation, etc. I aim for fast and reliable networks for autonomous driving, large language models, e-commerce, smart healthcare, among others.
              </p>

              <p>
                We always welcome great research interns. Reach out to us if interested!
              </p>

            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <strong>[2023 Feb]</strong> We will host the <strong>Full-stack Neural Network Acceleration Tutorial</strong> at <strong>CVPR 2023</strong>. See you in Vancouver!
              </p>
              <p>
                <strong>[2022 Nov]</strong> Featured in 30Kr 2022 Global Outstanding Chinese Power 100 Awards.
              </p>
              <p>
                <strong>[2022 Oct]</strong> I will be panelist at <strong>OCP Global Summit 22</strong>. See you in San Jose.
              </p>
              <p>
                <strong>[2022 Oct]</strong> I will be at Forbes Gala 22. Congrats to new awardees and see you in San Francisco.
              </p>
              <p>
                <strong>[2022 Sep]</strong> One paper accepted at <strong>BMVC</strong> on feature map inversion.
              </p>
              <p>
                <strong>[2022 Sep]</strong> One paper accepted at <strong>NeurIPS</strong> on hardware-aware pruning.
              </p>
              <p>
                <strong>[2022 Jul]</strong> I will be giving a keynote at <strong>DAC'60</strong> on efficient and secure deep learning. See you in San Francisco.
              </p>
              <p>
                <strong>[2022 Jul]</strong> One paper accepted at <strong>ECCV</strong> on hardware-aware model adaptation.
              </p>
              <p>
                <strong>[2022 Jun]</strong> I will be at CVPR in person. See you in New Orleans.
              </p>
              <p>
                <strong>[2022 Feb]</strong> Three papers accepted at <strong>CVPR</strong> and two papers at workshops on efficient and secure CNNs and ViTs.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publication</heading>
              <p>
                Partial, full list <a href="https://scholar.google.com/citations?user=4gdSoOYAAAAJ&hl=en">here</a> and in   <a href="data/hongxu_cv.pdf">CV</a>. <strong>*</strong> equal contribution.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="finv_stop()" onmouseover="finv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/feature-invert.png' width="160"></div>
                  <img src='images/feature-invert.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('finv_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('finv_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Privacy Vulnerability of Split Computing to Data-Free Model Inversion Attacks</papertitle>
              </a>
              <br>
              Xin Dong, <strong>Hongxu Yin</strong>, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov, H. T. Kung
              <br>
               <em>BMVC</em>, 2022
              <br>
              <a href="">code (to come)</a>
              /
              <a href="">arXiv (to come)</a>
              <p>
              We show viability to train an invert network that maps intermediate tensors back to inputs. Works smoothingly for GANs and classifiers on high resolution tasks.
            </p>
            </td>
          </tr>


          <tr onmouseout="lasp_stop()" onmouseover="lasp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/lasp.png' width="160"></div>
                  <img src='images/lasp.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('lasp_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('lasp_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Structural Pruning via Latency-Saliency Knapsack</papertitle>
              </a>
              <br>
              Maying Shen*, <strong>Hongxu Yin*</strong>, Pavlo Molchanov, Lei Mao, Jianna Liu, Jose M. Alvarez
              <br>
               <em>NeurIPS</em>, 2022
              <br>
              <a href="">code (to come)</a>
              /
              <a href="">arXiv (to come)</a>
              <p></p>
              <p>
              Sturcutral pruning is a quick knapsack problem to maximize accuracy through combining latency-guided parameter chunks.
            </p>
            </td>
          </tr>

          <tr onmouseout="avit_stop()" onmouseover="avit_start()"    bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/avit.png' width="160"></div>
                  <img src='images/avit.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('avit_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('avit_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yin_A-ViT_Adaptive_Tokens_for_Efficient_Vision_Transformer_CVPR_2022_paper.html">
                <papertitle>A-ViT: Adaptive Tokens for Efficient Vision Transformer</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, Pavlo Molchanov
              <br>
               <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://a-vit.github.io/">project page</a>
              /
              <a href="https://github.com/NVlabs/A-ViT">code</a>
              /
              <a href="https://arxiv.org/abs/2112.07658">arXiv</a>
              <p></p>
              <p>
              We show that transformers can quickly drop redundant tokens and reserve computation on only informative ones, offering off-the-shelf cost saving.
              </p>
            </td>
          </tr>


          <tr onmouseout="gradvit_stop()" onmouseover="gradvit_start()"   >
            <td style="padding:20px;width:15%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/gradvit.png' width="140"></div>
                  <img src='images/gradvit.png' width="140">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('avit_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('avit_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:60%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.11894">
                <papertitle>GradViT: Gradient Inversion of Vision Transformers</papertitle>
              </a>
              <br>
              Ali Hatamizadeh*, <strong>Hongxu Yin*</strong>, Holger Roth, Wenqi Li, Jan Kautz, Daguang Xu, Pavlo Molchanov
              <br>
               <em>CVPR</em>, 2022
              <br>
              <a href="https://gradvit.github.io/">project page</a>
              /
              <a>code (to come)</a>
              /
              <a href="https://arxiv.org/abs/2203.11894">arXiv</a>
              <p></p>
              <p>
              We show Vision Transformer gradient encode sufficient information such that private original images can be easily reconstructed via inversion.
              </p>
            </td>
          </tr>


          <tr onmouseout="whentoprune_stop()" onmouseover="whentoprune_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/whentoprune.png' width="160"></div>
                  <img src='images/whentoprune.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('avit_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('avit_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:60%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shen_When_To_Prune_A_Policy_Towards_Early_Structural_Pruning_CVPR_2022_paper.pdf">
                <papertitle>When To Prune? A Policy Towards Early Structural Pruning</papertitle>
              </a>
              <br>
              Maying Shen, Pavlo Molchanov, <strong>Hongxu Yin</strong>, Jose M. Alvarez
              <br>
               <em>CVPR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2110.12007.pdf">arXiv</a>
              <p></p>
              <p>
              We push structural pruning into earlier training, cutting down on training costs.
              </p>
            </td>
          </tr>


          <tr onmouseout="hant_stop()" onmouseover="hant_start()"   >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/hant.png' width="160"></div>
                  <img src='images/hant.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('hant_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('hant_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2107.10624">
                <papertitle>HANT: Hardware-aware Network Transformation</papertitle>
              </a>
              <br>
              Pavlo Molchanov*,
              Jimmy Hall*,
              <strong>Hongxu Yin*</strong>,
              Nicolo Fusi, Jan Kautz, Arash Vahdat
              <br>
               <em>ECCV (to appear)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2107.10624">arXiv</a>
              <p></p>
              <p>
                We argue for a Train-Large-Swap-Faster model acceleration paradigm. Quickly adapting a large model to varying constraints in CPU-second search yields the quick finding of Pareto front.
              </p>
            </td>
          </tr>

          <tr onmouseout="gradinv_stop()" onmouseover="gradinv_start()"   bgcolor="#ffffd0"  >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/gradinv.png' width="160"></div>
                  <img src='images/gradinv.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('gradinv_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('gradinv_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yin_See_Through_Gradients_Image_Batch_Recovery_via_GradInversion_CVPR_2021_paper.pdf">
                <papertitle>See through Gradients: Image Batch Recovery via GradInversion</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov
              <br>
               <em>CVPR</em>, 2021
              <br>
              <a href="">code (to come)</a>
              /
              <a href="https://arxiv.org/abs/2104.07586">arXiv</a>
              <p></p>
              <p>
              We show under strong inversion, gradients are in essence original data via inversion, even for large datasets, large nets, for high resolution.
              </p>
            </td>
          </tr>


          <tr onmouseout="quant_stop()" onmouseover="quant_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/quant.png' width="160"></div>
                  <img src='images/quant.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('quant_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('quant_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Idelbayev_Optimal_Quantization_Using_Scaled_Codebook_CVPR_2021_paper.pdf">
                <papertitle>Optimal Quantization using Scaled Codebook</papertitle>
              </a>
              <br>
              Yerlan Idelbayev, Pavlo Molchanov, Maying Shen, <strong>Hongxu Yin</strong>, Miguel A Carreira-Perpin√°n, Jose M Alvarez
              <br>
               <em>CVPR</em>, 2021
              <br>
              <p></p>
              <p>
              We aim at code-book oriented best scaled optimal quantization for deep nets.
              </p>
            </td>
          </tr>



          <tr onmouseout="di_stop()" onmouseover="di_start()"    bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/di.png' width="160"></div>
                  <img src='images/di.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('di_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('di_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.08795">
                <papertitle>Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Pavlo Molvhanov,
              Jose M. Alvarez,
              Zhizhong Li,
              Arun Mallya,
              Derek Hoiem,
              Niraj K. Jha,
              Jan Kautz
              <br>
               <em>CVPR</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://github.com/NVlabs/DeepInversion">code</a>
              /
              <a href="https://arxiv.org/abs/1912.08795">arXiv</a>
              <p></p>
              <p>
              We show that trained deep nets are in essence datasets. One can quickly invert from net outputs to synthesize a new dataset for off-the-shelf models. See ResNet-50 dreamed objects as left.
              </p>
            </td>
          </tr>


          <tr onmouseout="chamnet_stop()" onmouseover="chamnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/chamnet.png' width="160"></div>
                  <img src='images/chamnet.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('hlstm_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('hlstm_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1812.08934">
                <papertitle>ChamNet: Towards Efficient Network Design through Platform-Aware Model Adaptation</papertitle>
              </a>
              <br>
              Xiaoliang Dai, Peizhao Zhang, Bichen Wu, <strong>Hongxu Yin</strong>, Fei Sun, Yanghan Wang, Marat Dukhan, Yunqing Hu, Yiming Wu, Yangqing Jia, Peter Vajda, Matt Uyttendaele, Niraj K. Jha
              <br>
               <em>CVPR</em>, 2019
              <br>
              <a href="https://github.com/facebookresearch/mobile-vision">code</a>
              /
              <a href="https://arxiv.org/abs/1812.08934">arXiv</a>
              <p></p>
              <p>
              A genetic algorithm to quickly adjust the hyper architecture of a base model to target platforms (DSP, CPU, GPU) given constraints such as latency or memory.
              </p>
            </td>
          </tr>



          <tr onmouseout="diabdeep_stop()" onmouseover="diabdeep_start()"  >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/diabdeep.png' width="160"></div>
                  <img src='images/diabdeep.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('diabedeep_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('diabedeep_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.04925">
                <papertitle>DiabDeep: Pervasive Diabetes Diagnosis based on Wearable Medical Sensors and Efficient Neural Networks</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Bilal Mukadam, Xiaoliang Dai, Niraj K. Jha
              <br>
               <em>IEEE Trans. Emerging Topics in Computing</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1910.04925">arXiv</a>
              <p></p>
              <p>
              Wearable medical sensors, backed by extremely efficient NNs, offers around-the-clock diabetes diagnosis.
              </p>
            </td>
          </tr>



          <tr onmouseout="hlstm_stop()" onmouseover="hlstm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/hlstm.png' width="160"></div>
                  <img src='images/hlstm.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('hlstm_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('hlstm_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1805.11797">
                <papertitle>Grow and Prune Compact, Fast, and Accurate LSTMs</papertitle>
              </a>
              <br>
              Xiaoliang Dai*,
              <strong>Hongxu Yin*</strong>, Niraj K Jha
              <br>
               <em>IEEE Trans. Computers</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1805.11797">arXiv</a>
              <p></p>
              <p>
              We show that grow-and-prune yields faster yet more accurate H-LSTM family, surpassing LSTMs and GRUs.
              </p>
            </td>
          </tr>


          <tr onmouseout="hardhlstm_stop()" onmouseover="hardhlstm_start()"  >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/hardhlstm.png' width="160"></div>
                  <img src='images/hardhlstm.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('di_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('di_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9556553">
                <papertitle>Towards Execution-Efficient LSTMs via Hardware-Guided Grow-and-Prune Paradigm</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Guoyang Chen, Yingmin Li, Shuai Che, Weifeng Zhang, Niraj K Jha
              <br>
               <em>IEEE Trans. Emerging Topics in Computing</em>, 2019
              <br>
              <a href="https://ieeexplore.ieee.org/document/9556553">arXiv</a>
              <p></p>
              <p>
              We observe high degree of non-monoticity in latency surface given shrinking model dimensions, and propose a systematic structural Grow-and-Prune way to exlpoit this for faster inference.
              </p>
            </td>
          </tr>



          <tr onmouseout="nest_stop()" onmouseover="nest_start()"   >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/nest.png' width="160"></div>
                  <img src='images/nest.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('hlstm_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('hlstm_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1711.02017">
                <papertitle>NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm</papertitle>
              </a>
              <br>
              Xiaoliang Dai,
              <strong>Hongxu Yin</strong>, Niraj K Jha
              <br>
               <em>IEEE Trans. Computers</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1711.02017">arXiv</a>
              <p></p>
              <p>
              Human brains grow before age 2 before pruning neurons for efficient synapsis afterwards. We propose grow-and-prune accordingly, and show consistent improvements over conventional pruning that always start from full models.
              </p>
            </td>
          </tr>





          <tr onmouseout="smarthealthcare_stop()" onmouseover="smarthealthcare_start()  ">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/smarthealthcare.png' width="160"></div>
                  <img src='images/smarthealthcare.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('smarthealthcare_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('smarthealthcare_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.amazon.com/Healthcare-Foundations-Trends-Electronic-Automation/dp/1680834401">
                <papertitle>Smart Healthcare</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Ayten Ozge Akmandor, Niraj K. Jha
              <br>
               <em>Foundations & Trends</em>, 2017, &nbsp <font color="red"><strong>(Book chapter)</strong></font>
              <br>
              <a href="https://www.amazon.com/Healthcare-Foundations-Trends-Electronic-Automation/dp/1680834401">arXiv</a>
              <p></p>
              <p>
              We aim to lay out foundations for pervasive healthcare from a wearables angle. Book now available at <a href="https://www.amazon.com/Healthcare-Foundations-Trends-Electronic-Automation/dp/1680834401">Amazon</a>.
            </p>
            </td>
          </tr>



          <tr onmouseout="fpga_stop()" onmouseover="fpga_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/fpga.png' width="160"></div>
                  <img src='images/fpga.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('fpga_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('fpga_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/7168793">
                <papertitle>Novel Real-time System Design for Floating-point sub-Nyquist Multi-coset Signal Blind reconstruction</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Bah Hwee Gwee, Zhiping Lin, Anil Kumar, Sirajudeen Gulam Razul, Chong Meng Samson See
              <br>
               <em>ISCAS</em>, 2015 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://ieeexplore.ieee.org/document/7168793">arXiv</a>
              <p></p>
              <p>
              An FPGA solution augmented by a novel real-time tri-core SVD design for multi-coset signal reconstruction.
            </p>
            </td>
          </tr>


        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Patents</heading>
              <p>Full list in <a href="data/hongxu_cv.pdf">CV</a>. </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Invited Talks</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
              &#9826; Towards Efficient and Secure Deep Learning, <em>Invited Keynote, Design & Automation Conference (DAC'60)</em>
              <br>
              &#9826; Towards Efficient and Secure Deep Nets, <em>University of British Columbia ECE Department</em>
              <br>
              &#9826; Inverting Deep Nets, <em>Princeton University, Department of Computer Science research groups</em>
              <br>
              &#9826; See through Gradients, <em>Europe ML meeting</em>
              <br>
              &#9826; Dreaming to Distill, <em>Synced AI (Êú∫Âô®‰πãÂøÉ)</em>
              <br>
              &#9826; Dreaming to Distill, <em>Facebook AR/VR</em>
              <br>
              &#9826; Making Neural Networks Efficient, <em>Alibaba Cloud / Platform AI group</em>
              <br>
              &#9826; Efficient Neural Networks, <em>Efficient Neural Networks</em>
              <br>
              &#9826; Efficient Neural Networks, <em>Baidu Research, ByteDance A.I. Lab US</em>
              <br>
              &#9826; Efficient Neural Networks, <em>Alibaba A.I. Research, Kwai Lab</em>
              <br>
              &#9826; Applied Machine Learning: From Theory to Practice, <em>Invited Keynote, IEEE Circuits and Systems Society (Singapore Chapter)</em>
              <br>
              &#9826; A Health Decision Support System for Disease Diagnosis, <em> New Jersey Tech Council</em>
              </td>
          </tr>

        </tbody></table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Reviewer & Committee</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
              <strong><em> (Conferences) </em></strong>
              <br>
              <br>
              &#9826; Computer Vision and Pattern Recognition (CVPR)
              <br>
              &#9826; International Conference on Computer Vision (ICCV)
              <br>
              &#9826; Conference on Neural Information Processing Systems (NeurIPS)
              <br>
              &#9826; International Conference on Machine Learning (ICML)
              <br>
              &#9826; European Conference on Computer Vision (ECCV)
              <br>
              &#9826; British Machine Vision Conference (BMVC)
              <br>
              &#9826; Winter Conference on Applications of Computer Vision (WACV)
              <br>
              &#9826; AAAI Conference on Artificial Intelligence (AAAI)
              <br>
              &#9826; Design Automation Conference (DAC)
              <br>
              &#9826; High-Performance Computer Architecture (HPCA)
              <br>
              <br>
              <br>
              <strong><em> (Journals) </em></strong>
              <br>
              <br>
              &#9826; IEEE Transactions on Pattern Analysis and Machine Intelligence
              <br>
              &#9826; IEEE Transactions on Neural Networks and Learning Systems
              <br>
              &#9826; International Journal of Computer Vision
              <br>
              &#9826; IEEE Journal of Biomedical and Health Informatics
              <br>
              &#9826; IEEE Journal of Selected Topics in Signal Processing
              <br>
              &#9826; IEEE Sensors Journal
              <br>
              &#9826; IEEE Consumer Electronics Magazine
              <br>
              &#9826; International Journal on Artificial Intelligence Tools
              <br>
              &#9826; International Journal of Systems Architecture
              <br>
              &#9826; International Journal of Healthcare Technology and Management
              <br>
              &#9826; International Journal of Electronic Imaging
              <br>

              </td>
          </tr>

        </tbody></table>




        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Mentorship</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
              <strong><em> (Princeton Senior Thesis Mentees) </em></strong>
              <br>
              <br>
              &#9826; Joe Zhang, now Ph.D. at Stanford
              <br>
              &#9826; Hari Santhanam, now Ph.D. at University of Pennsylvania
              <br>
              &#9826; Frederick Hertan, now at SIG Trading
              <br>
              &#9826; Kyle Johnson, now at Princeton University
              <br>
              &#9826; Bilal Mukadam, now at Microsoft
              <br>
              &#9826; Chloe Song, now at Astra Inc.
              <br>
              <br>
              <br>
              <strong><em> (NVIDIA Research Interns) </em></strong>
              <br>
              <br>
              &#9826; Divyam Madaan, Ph.D., New York University
              <br>
              &#9826; Shixing Yu, Ph.D., University of Texas, Austin
              <br>
              &#9826; Annamarie Bair, Ph.D., Carnegie Mellon University
              <br>
              &#9826; Alex Sun, B.E., University of Illinois Urbana-Champaign
              <br>
              &#9826; Huanrui Yang, Ph.D., Duke University
              <br>
              &#9826; Zhen Dong, Ph.D., University of California, Berkeley
              <br>
              &#9826; Xin Dong, Ph.D., Harvard University
              <br>
              &#9826; Paul Micaelli, Ph.D., University of Edingbugh
              <br>
              &#9826; Yerlan Idelbayev, Ph.D., University of California, Merced
              <br>
              &#9826; Vu Nguyen, Ph.D., Stony Brooks University
              <br>
              &#9826; Akshay Chawla, M.E., Carnegie Mellon University
              <br>

              </td>
          </tr>

        </tbody></table>


        <table width="100%" align="right" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr align="right">
            <td>(web template from <a href="https://jonbarron.info/">here</a>, with thanks)</td>
          </tr>

        </tbody></table>


      </td>
    </tr>





  </table>
</body>

</html>
