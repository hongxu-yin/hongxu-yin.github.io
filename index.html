<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hongxu (Danny) Yin</title>

  <meta name="author" content="Hongxu (Danny) Yin">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hongxu (Danny) Yin</name>
              </p>

              <p>
                I‚Äôm a Tech Lead at NVIDIA, working on multimodal LLMs. I‚Äôve led projects across the VILA model family, focusing on post-training, agent capabilities, and multimodal reasoning, with full-stack optimization for NVIDIA hardware.</p>

              <p>I joined NVIDIA Research after completing my Ph.D. at Princeton University, as part of the Learning and Perception Research Team.
              </p>
                <p>
                  I was named one of Forbes Top 60 Elite Chinese in North America and received 36Kr Global Outstanding Chinese Power 100 Award.
                  <!-- I am a recipient of Princeton Yan Huo 94* Graduate Fellowship, Princeton ECE Best Dissertation Finalist, Princeton Natural Sciences and Engineering Fellowship, Defense Science & Technology Agency gold medal, and Thomson Asia Pacific Holdings gold medal. -->

                </p>
                <!-- <p>I have also been featured by <a href="https://www.forbes.com/">Forbes</a> as Top 60 Elite Chinese in North America and  <a href="https://36kr.com/">36Kr</a> with Global Outstanding Chinese Power 100 Award.
              </p> -->
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=4gdSoOYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/yin_hongxu">Twitter</a> /&nbsp -->
                <a href="data/hongxu_cv.pdf">CV</a> &nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/danny_yin_photo.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/danny_yin_photo.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">

              <heading>Research Overview</heading>


              <p>
                <!-- I am TL for NVIDIA‚Äôs herd of multimodal models (VILA and its model family series) on post-training, agent, and reasoning aspects of multimodal LLMs, with full-stack optimization for NVIDIA's hardware.</p> -->

              <!-- <p>
                I'm interested in a deeper understanding of neural nets. This makes neural net more capable, efficient and secure. Topics cover resource efficient (data/computation) deep learning, overseeing CNNs and transformers, leveraging model
                inversion, knowledge distillation, dynamic inference, neural architecture search, pruning, quantization, model adaptation, etc.
              </p>

              <p>
                I aim for fast and reliable networks for multi-modality tasks, autonomous driving, large language models, e-commerce, smart healthcare, among others.
              </p> -->

             <p>
                <!-- I am currently co-leading the <a href="https://github.com/NVlabs/VILA">VILA</a> project.  -->
                For intern/full-timer to join NVIDIA's multimodal efforts, please send a CV to dannyy [at] nvidia [dot] com.
              </p>
<!--
              <p>
              <strong><font color="green"> Dec. 2023 - I have research intern slots for VLM next spring/summer/fall 2024, and please drop a CV if interested. </strong>
              </p> -->


                            <img src='images/vila-summary.png' width="660">



            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>


              <p>
                <strong>[2025 Jun]</strong> We just released VILA-based VLAs for robots for navigation <font color="green"><strong>(NaVILA)</strong></font> and manipulation <font color="green"><strong>(CoT-VLA and EgoVLA)</strong></font>.
              </p>

              <p>
                <strong>[2025 Feb]</strong> Four papers accepted at <font color="green"><strong>CVPR 2025</strong></font> on LLM and VLM, of which two as <font color="red"><strong>Highlight</strong></font> (VILA Medical Agent, and VILA-HD Agent, NVILA/VILA2.0 base, and C-RADIO v2 vision encoder).
              </p>

              <p>
                 <strong>[2025 Feb]</strong> We will host the <font color="green"><strong>Efficient Computer Vision workshop</strong></font> and <font color="green"><strong>GPU-based DL Acceleration tutorial</strong></font> at <font color="green"><strong>CVPR 2025</strong></font>. See you in Nashville.
               </p>

              <p>
                <strong>[2025 Jan]</strong> Four papers accepted at <font color="green"><strong>ICLR 2025</strong></font> on LLM and VLM, of which one as <font color="red"><strong>Spotlight</strong></font>.
              </p>

             <!-- <p>
                <strong>[2024 Dec]</strong> We will host the <font color="green"><strong>Efficient Computer Vision workshop</strong></font> at <font color="green"><strong>CVPR 2025</strong></font>. See you in Nashville.
              </p> -->

              <p>
                <strong>[2024 Sep]</strong> Two papers accepted at <font color="green"><strong>NeurIPS 2024</strong></font> on LLM and VLM, of which one as <font color="red"><strong>Spotlight</strong></font>.
              </p>
              <p>
                <strong>[2024 May]</strong> Three papers accepted at <font color="green"><strong>ICML 2024</strong></font> on efficient and secure LLMs, of which two as <font color="red"><strong>Oral (top 1.5%)</strong></font>.
              </p>
              <p>
                <strong>[2024 Apr]</strong> We will host the <font color="green"><strong>Efficient Foundation Model workshop</strong></font> at <font color="green"><strong>ECCV 2024</strong></font>. See you in Milan.
              </p>
              <p>
                <strong>[2024 Mar]</strong> FedBPT won the <font color="red"><strong>Best Paper Award</strong></font> at <font color="green"><strong>AAAI symposium</strong></font>. Congrats to Jingwei!
              </p>
              <p>
                <strong>[2024 Feb]</strong> Two papers accepted at <font color="green"><strong>CVPR 2024</strong></font> on VLMs.
              </p>
              <!-- <p>
                <strong>[2024 Feb]</strong> We will host the <font color="green"><strong>Efficient Computer Vision workshop</strong></font> and <font color="green"><strong>GPU-based DL Acceleration tutorial</strong></font> at <font color="green"><strong>CVPR 2024</strong></font>. See you in Vancouver.
              </p>
              <p>
                <strong>[2024 Jan]</strong> Two papers accepted at <font color="green"><strong>ICLR 2024</strong></font> on efficient and robust large models.
              </p> -->
              <!-- <p>
                <strong>[2023 Apr]</strong> Two papers accepted at <font color="green"><strong>ICML 2023</strong></font> on improving transformer and diffusion models.
              </p>
              <p>
                <strong>[2023 Apr]</strong> We will host the <font color="green"><strong>Data-efficient Learning Tutorial</strong></font> at <font color="green"><strong>ICCV 2023</strong></font>. See you in Paris.
              </p>
              <p>
                <strong>[2023 Mar]</strong> Three papers accepted at <font color="green"><strong>CVPR 2023</strong></font> on resource efficient deep learning.
              </p>
              <p>
                <strong>[2023 Feb]</strong> We will host the <font color="green"><strong>GPU-based Acceleration Tutorial</strong></font> at <font color="green"><strong>CVPR 2023</strong></font>. See you in Vancouver.
              </p>
              <p>
                <strong>[2022 Nov]</strong> Featured in <font color="green"><strong>36Kr</strong></font> 2022 Global Outstanding Chinese Power 100 Awards.
              </p>
              <p>
                <strong>[2022 Oct]</strong> I will be at <font color="green"><strong>Forbes Dinner 2022</strong></font>. Congrats to new awardees and see you in San Francisco.
              </p> -->
              <!-- <p>
                <strong>[2022 Sep]</strong> One paper accepted at <font color="green"><strong>BMVC 2022</strong></font> on feature map inversion.
              </p>
              <p>
                <strong>[2022 Sep]</strong> One paper accepted at <font color="green"><strong>NeurIPS 2022</strong></font> on hardware-aware pruning.
              </p>
              <p>
                <strong>[2022 Jul]</strong> I will give a keynote at <font color="green"><strong>DAC'60</strong></font> on efficient and secure deep learning. See you in San Francisco.
              </p>
              <p>
                <strong>[2022 Jul]</strong> One paper accepted at <font color="green"><strong>ECCV 2022</strong></font> on hardware-aware model adaptation.
              </p>
              <p>
                <strong>[2022 Feb]</strong> Three papers accepted at <font color="green"> <strong>CVPR 2022</strong></font> and two papers at workshops on efficient and secure CNNs and ViTs.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>(Partial) Publications</heading>
              <p>
                Full list <a href="https://scholar.google.com/citations?user=4gdSoOYAAAAJ&hl=en">here</a>. <strong>*</strong> equal contribution. ^ interns. &Dagger; equal advisory.
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>




                    <tr onmouseout="vila_stop()" onmouseover="vila_start()"    bgcolor="e8f7f8">
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <div class="two" id='di_image'>
                            <img src='images/vila.png' width="160"></div>
                            <img src='images/vila.png' width="160">
                        </div>
                        <script type="text/javascript">
                          function malle_start() {
                            document.getElementById('vila_image').style.opacity = "1";
                          }

                          function malle_stop() {
                            document.getElementById('vila_image').style.opacity = "0";
                          }
                          malle_stop()
                        </script>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2312.07533">
                          <papertitle>VILA: On Pre-training for Visual Language Models</papertitle>
                        </a>
                        <br>
                        Ji Lin*^, <strong>Hongxu Yin*</strong>, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han
                        <br>
                         <em>CVPR</em>, 2024
                        <br>
                        <a href="https://arxiv.org/abs/2312.07533">arXiv</a> /
                        <a href="https://github.com/NVlabs/VILA">code</a> /
                        <a href="https://developer.nvidia.com/blog/visual-language-models-on-nvidia-hardware-with-vila/">NVIDIA Technical Blog</a> /
                        <a href="https://developer.nvidia.com/blog/visual-language-intelligence-and-edge-ai-2-0/">NVIDIA Jetson Tutorial</a>
                        <p></p>
                        <p>
                        With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles.
                        </p>
                      </td>
                    </tr>



                    <tr onmouseout="navila_stop()" onmouseover="navila_start()"  bgcolor="e8f7f8">
                      <td style="padding:20px;width:15%;vertical-align:middle">
                        <div class="one">
                          <div class="two" id='di_image'>
                            <img src='images/navila.png' width="160"></div>
                            <img src='images/navila.png' width="160">
                        </div>
                        <script type="text/javascript">
                          function malle_start() {
                            document.getElementById('navila_image').style.opacity = "1";
                          }

                          function malle_stop() {
                            document.getElementById('navila_image').style.opacity = "0";
                          }
                          malle_stop()
                        </script>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2412.04453">
                          <papertitle>NaVILA: Legged Robot Vision-Language-Action Model for Navigation</papertitle>
                        </a>
                        <br>
                        An-Chieh Cheng*, Yandong Ji*, Zhaojing Yang*, Xueyan Zou, Jan Kautz, Erdem Bƒ±yƒ±k, <strong>Hongxu Yin&Dagger;</strong>, Sifei Liu&Dagger;, Xiaolong Wang&Dagger;
                        <br>
                         <em>RSS</em>, 2025
                        <br>
                        <a href="https://arxiv.org/abs/2412.04453">arXiv</a>
                        <p></p>
                        <p>
                          VILA navigation model for humanoid and other legged robots with real-time deployment.
                        </p>
                      </td>
                    </tr>








              <tr onmouseout="nvila_stop()" onmouseover="nvila_start()"    bgcolor="#e8f7f8">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='di_image'>
                      <img src='images/nvila.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('nvila_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('nvila_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2412.04468">
                    <papertitle>NVILA: Efficient Frontier Visual Language Models</papertitle>
                  </a>
                  <br>
                  Zhijian Liu*, Ligeng Zhu*, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, <strong>Hongxu Yin&Dagger;</strong>, Song Han&Dagger;, Yao Lu&Dagger;
                  <br>
                   <em>CVPR</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2412.04468">arXiv</a> /
                  <a href="https://github.com/NVlabs/VILA">code & checkpoints</a>
                  <p></p>
                  <p>
                    Efficient frontier VLM models with efficient training and inference.
                  </p>
                </td>
              </tr>





                      <tr onmouseout="vilahd_stop()" onmouseover="vilahd_start()"    bgcolor="e8f7f8">
                        <td style="padding:20px;width:25%;vertical-align:middle">
                          <div class="one">
                            <div class="two" id='di_image'>
                              <img src='images/vila-hd.png' width="160"></div>
                              <img src='images/vila-hd.png' width="160">
                          </div>
                          <script type="text/javascript">
                            function malle_start() {
                              document.getElementById('vila-hd_image').style.opacity = "1";
                            }

                            function malle_stop() {
                              document.getElementById('xvil-hd_image').style.opacity = "0";
                            }
                            malle_stop()
                          </script>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                          <a href="https://arxiv.org/abs/2503.19903">
                            <papertitle>VILA-HD: Scaling Vision Pre-Training to 4K Resolution</papertitle>
                          </a>
                          <br>
                          Baifeng Shi, Boyi Li, Han Cai, Yao Lu, Sifei Liu, Marco Pavone, Jan Kautz, Song Han, Trevor Darrell, Pavlo Molchanov, <strong>Hongxu Yin</strong>
                          <br>
                           <em>CVPR</em>, 2025&nbsp<font color="red"><strong>(Highlight paper)</strong></font>
                          <br>
                          <a href="https://arxiv.org/abs/2503.19903">arXiv</a>/
                          <a href="https://nvlabs.github.io/PS3/">code & checkpoints</a>
                          <p></p>
                          <p>
                            We introduce VILA-HD, an adaptive architecture that self-focuses on area of interest depending on user prompts and cuts down on computations by >10x.
                          </p>
                        </td>
                      </tr>




          <tr onmouseout="xvila_stop()" onmouseover="xvila_start()"    bgcolor="e8f7f8">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/xvila.png' width="160"></div>
                  <img src='images/xvila.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('xvila_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('xvila_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.07533">
                <papertitle>X-VILA: Cross-Modality Alignment for Large Language Model</papertitle>
              </a>
              <br>
              Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, <strong>Hongxu Yin</strong>
              <br>
               <em>preprint</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2405.19335">arXiv</a>
              <p></p>
              <p>
                We introduce X-VILA, a foundation model for cross-modality understanding, reasoning, and generation in the domains of video, image, language, and audio. X-VILA demonstrates the ability to perceive (see, hear, and read) multi-modality inputs, and generate (draw, speak, and write) multi-modality responses.
              </p>
            </td>
          </tr>



                    <tr onmouseout="vila2_stop()" onmouseover="vila2_start()">
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <div class="two" id='di_image'>
                            <img src='images/vila2.png' width="160"></div>
                            <img src='images/vila2.png' width="160">
                        </div>
                        <script type="text/javascript">
                          function malle_start() {
                            document.getElementById('vila2_image').style.opacity = "1";
                          }

                          function malle_stop() {
                            document.getElementById('vila2_image').style.opacity = "0";
                          }
                          malle_stop()
                        </script>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2312.07533">
                          <papertitle>VILA<sup>2</sup>: VILA Augmented VILA</papertitle>
                        </a>
                        <br>
                        Yunhao Fang^*, Ligeng Zhu*, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, <strong>Hongxu Yin</strong>
                        <br>
                         <em>preprint</em>, 2024
                        <br>
                        <a href="https://arxiv.org/abs/2407.17453">arXiv</a>
                        <p></p>
                        <p>
                          We observe three rounds of free-lunch for VLM boosting, followed by a novel specialist augmentation mechanism.
                        </p>
                      </td>
                    </tr>


          <tr onmouseout="dora_stop()" onmouseover="dora_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/dora.jpeg' width="160"></div>
                  <img src='images/dora.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('dora_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('dora_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.07533">
                <papertitle>DoRA: Weight-Decomposed Low-Rank Adaptation</papertitle>
              </a>
              <br>
              Shih-Yang Liu, Chien-Yi Wang, <strong>Hongxu Yin</strong>, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Min-Hung Chen
              <br>
               <em>ICML</em>, 2024 <font color="red"><strong>(Oral - top 1.5%)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2312.07533">arXiv</a> /
              <a href="https://github.com/NVlabs/DoRA">code</a>
              <p></p>
              <p>
              DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding.
              </p>
            </td>
          </tr>





        <tr onmouseout="df_stop()" onmouseover="df_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/regionGPT.png' width="160"></div>
                  <img src='images/regionGPT.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('regionGPT_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('regionGPT_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2403.02330">
                <papertitle>RegionGPT: Towards Region Understanding Vision Language Model</papertitle>
              </a>
              <br>
              Qiushan Guo^, Shalini d'Mello*, <strong>Hongxu Yin*</strong>, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, Sifei Liu
              <br>
               <em>CVPR</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2403.02330">arXiv</a>
              <p></p>
              <p>
                We introduce RegionGPT that enables complex region-level captioning, reasoning, classification, and expression comprehension capabilities for the multimodal large language model.
              </p>
            </td>
          </tr>



        <tr onmouseout="df_stop()" onmouseover="df_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='di_image'>
                <img src='images/cdf.png' width="160"></div>
                <img src='images/cdf.png' width="160">
            </div>
            <script type="text/javascript">
              function malle_start() {
                document.getElementById('cdf_image').style.opacity = "1";
              }

              function malle_stop() {
                document.getElementById('cdf_image').style.opacity = "0";
              }
              malle_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="">
              <papertitle>Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation</papertitle>
            </a>
            <br>
            Jiaming Song, Qinsheng Zhang, <strong>Hongxu Yin</strong>, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, Arash Vahdat
            <br>
             <em>ICML</em>, 2023
            <br>
            <a href="https://openreview.net/pdf?id=JzZ2xAvCs8">arXiv</a>
            <p></p>
            <p>
            Diffusion model for Plug-and-Play controllable generation. </p>
          </td>
        </tr>

          <tr onmouseout="hcl_stop()" onmouseover="hcl_start()"    bgcolor="e8f7f8">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/HCL.png' width="160"></div>
                  <img src='images/HCL.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('hcl_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('hcl_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Heterogeneous Continual Learning</papertitle>
              </a>
              <br>
              Divyam Madaan^, <strong>Hongxu Yin</strong>, Wonmin Byeon, Jan Kautz, Pavlo Molchanov
              <br>
               <em>CVPR</em>, 2023 &nbsp <font color="red"><strong>(Highlight - top 2.5%)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2306.08593">arXiv</a> /
              <a href="https://github.com/NVlabs/HCL">code</a>
              <p></p>
              <p>
              Continual learning is now enabled for evolving model architecture upgrades. Your current model and new data are all you need to swap into a stronger architecture.
              </p>
            </td>
          </tr>

          <tr onmouseout="nvit_stop()" onmouseover="nvit_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/nvit.png' width="160"></div>
                  <img src='images/nvit.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('nvit_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('nvit_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Global Vision Transformer Pruning with Hessian-aware Saliency</papertitle>
              </a>
              <br>
              Huanrui Yang^, <strong>Hongxu Yin</strong>, Maying Shen, Pavlo Molchanov, Hai Li, Jan Kautz
              <br>
               <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2110.04869">arXiv</a> /
              <a href="https://github.com/NVlabs/NViT">code</a>
              <p></p>
              <p>
              Transformers are vastly redundant, and one-click global structural pruning offers speed up right away. Stacking same block across depth is lazy, and a new distribution rule is proposed based on embedding only.
              </p>
            </td>
          </tr>


          <tr onmouseout="nvit_stop()" onmouseover="nvit_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/deq.png' width="160"></div>
                  <img src='images/deq.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('deq_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('deq_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Recurrence without Recurrence: Stable Video Landmark Detection with Deep Equilibrium Models</papertitle>
              </a>
              <br>
              Paul Micaelli^, Arash Vahdat, <strong>Hongxu Yin</strong>, Jan Kautz, Pavlo Molchanov
              <br>
               <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2304.00600">arXiv</a> /
              <a href="">code (to come)</a>
              <p></p>
              <p>
              Deep equilibrium models smooth out flickering effect in video, enabled through recurrence without temporal-aware training, yielding efficient early stopping.
              </p>
            </td>
          </tr>


          <tr onmouseout="finv_stop()" onmouseover="finv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/feature-invert.png' width="160"></div>
                  <img src='images/feature-invert.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('finv_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('finv_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Privacy Vulnerability of Split Computing to Data-Free Model Inversion Attacks</papertitle>
              </a>
              <br>
              Xin Dong^, <strong>Hongxu Yin</strong>, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov, H. T. Kung
              <br>
               <em>BMVC</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2107.06304">arXiv</a> /
              <a href="">code (to come)</a>
              <p>
              We show viability to train an invert network that maps intermediate tensors back to inputs. Works smoothingly for GANs and classifiers on high resolution tasks.
            </p>
            </td>
          </tr>


          <tr onmouseout="lasp_stop()" onmouseover="lasp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/lasp.png' width="160"></div>
                  <img src='images/lasp.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('lasp_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('lasp_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Structural Pruning via Latency-Saliency Knapsack</papertitle>
              </a>
              <br>
              Maying Shen*, <strong>Hongxu Yin*</strong>, Pavlo Molchanov, Lei Mao, Jianna Liu, Jose M. Alvarez
              <br>
               <em>NeurIPS</em>, 2022
              <br>
              <a href="https://halp-neurips.github.io/">project page</a>
              /
              <a href="https://github.com/NVlabs/HALP">code</a>
              /
              <a href="https://arxiv.org/abs/2210.06659">arXiv</a>
              <p></p>
              <p>
              Sturcutral pruning is a quick knapsack problem to maximize accuracy through combining latency-guided parameter chunks.
            </p>
            </td>
          </tr>

          <tr onmouseout="avit_stop()" onmouseover="avit_start()"    bgcolor="e8f7f8">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/avit.png' width="160"></div>
                  <img src='images/avit.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('avit_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('avit_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yin_A-ViT_Adaptive_Tokens_for_Efficient_Vision_Transformer_CVPR_2022_paper.html">
                <papertitle>A-ViT: Adaptive Tokens for Efficient Vision Transformer</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, Pavlo Molchanov
              <br>
               <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral presentation)</strong></font>
              <br>
              <a href="https://a-vit.github.io/">project page</a>
              /
              <a href="https://github.com/NVlabs/A-ViT">code</a>
              /
              <a href="https://arxiv.org/abs/2112.07658">arXiv</a>
              <p></p>
              <p>
              We show that transformers can quickly drop redundant tokens and reserve computation on only informative ones, offering off-the-shelf cost saving.
              </p>
            </td>
          </tr>


          <tr onmouseout="gradvit_stop()" onmouseover="gradvit_start()"   >
            <td style="padding:20px;width:15%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/gradvit.png' width="140"></div>
                  <img src='images/gradvit.png' width="140">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('avit_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('avit_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:60%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.11894">
                <papertitle>GradViT: Gradient Inversion of Vision Transformers</papertitle>
              </a>
              <br>
              Ali Hatamizadeh*, <strong>Hongxu Yin*</strong>, Holger Roth, Wenqi Li, Jan Kautz, Daguang Xu, Pavlo Molchanov
              <br>
               <em>CVPR</em>, 2022
              <br>
              <a href="https://gradvit.github.io/">project page</a>
              /
              <a>code (to come)</a>
              /
              <a href="https://arxiv.org/abs/2203.11894">arXiv</a>
              <p></p>
              <p>
              We show Vision Transformer gradient encode sufficient information such that private original images can be easily reconstructed via inversion.
              </p>
            </td>
          </tr>


          <tr onmouseout="whentoprune_stop()" onmouseover="whentoprune_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/whentoprune.png' width="160"></div>
                  <img src='images/whentoprune.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('avit_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('avit_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:60%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shen_When_To_Prune_A_Policy_Towards_Early_Structural_Pruning_CVPR_2022_paper.pdf">
                <papertitle>When To Prune? A Policy Towards Early Structural Pruning</papertitle>
              </a>
              <br>
              Maying Shen, Pavlo Molchanov, <strong>Hongxu Yin</strong>, Jose M. Alvarez
              <br>
               <em>CVPR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2110.12007.pdf">arXiv</a>
              <p></p>
              <p>
              We push structural pruning into earlier training, cutting down on training costs.
              </p>
            </td>
          </tr>


          <tr onmouseout="hant_stop()" onmouseover="hant_start()"   >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/hant.png' width="160"></div>
                  <img src='images/hant.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('hant_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('hant_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2107.10624">
                <papertitle>HANT: Hardware-aware Network Transformation</papertitle>
              </a>
              <br>
              Pavlo Molchanov*,
              Jimmy Hall*,
              <strong>Hongxu Yin*</strong>,
              Nicolo Fusi, Jan Kautz, Arash Vahdat
              <br>
               <em>ECCV (to appear)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2107.10624">arXiv</a>
              <p></p>
              <p>
                We argue for a Train-Large-Swap-Faster model acceleration paradigm. Quickly adapting a large model to varying constraints in CPU-second search yields the quick finding of Pareto front.
              </p>
            </td>
          </tr>

          <tr onmouseout="gradinv_stop()" onmouseover="gradinv_start()"   bgcolor="e8f7f8"  >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/gradinv.png' width="160"></div>
                  <img src='images/gradinv.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('gradinv_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('gradinv_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yin_See_Through_Gradients_Image_Batch_Recovery_via_GradInversion_CVPR_2021_paper.pdf">
                <papertitle>See through Gradients: Image Batch Recovery via GradInversion</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov
              <br>
               <em>CVPR</em>, 2021
              <br>
              <a href="">code (to come)</a>
              /
              <a href="https://arxiv.org/abs/2104.07586">arXiv</a>
              <p></p>
              <p>
              We show under strong inversion, gradients are in essence original data via inversion, even for large datasets, large nets, for high resolution.
              </p>
            </td>
          </tr>


          <tr onmouseout="quant_stop()" onmouseover="quant_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/quant.png' width="160"></div>
                  <img src='images/quant.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('quant_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('quant_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Idelbayev_Optimal_Quantization_Using_Scaled_Codebook_CVPR_2021_paper.pdf">
                <papertitle>Optimal Quantization using Scaled Codebook</papertitle>
              </a>
              <br>
              Yerlan Idelbayev^, Pavlo Molchanov, Maying Shen, <strong>Hongxu Yin</strong>, Miguel A Carreira-Perpin√°n, Jose M Alvarez
              <br>
               <em>CVPR</em>, 2021
              <br>
              <p></p>
              <p>
              We aim at code-book oriented best scaled optimal quantization for deep nets.
              </p>
            </td>
          </tr>



          <tr onmouseout="di_stop()" onmouseover="di_start()"    bgcolor="e8f7f8">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/di.png' width="160"></div>
                  <img src='images/di.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('di_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('di_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.08795">
                <papertitle>Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Pavlo Molvhanov,
              Jose M. Alvarez,
              Zhizhong Li,
              Arun Mallya,
              Derek Hoiem,
              Niraj K. Jha,
              Jan Kautz
              <br>
               <em>CVPR</em>, 2020 &nbsp <font color="red"><strong>(Oral presentation)</strong></font>
              <br>
              <a href="https://github.com/NVlabs/DeepInversion">code</a>
              /
              <a href="https://arxiv.org/abs/1912.08795">arXiv</a>
              <p></p>
              <p>
              We show that trained deep nets are in essence datasets. One can quickly invert from net outputs to synthesize a new dataset for off-the-shelf models. See ResNet-50 dreamed objects as left.
              </p>
            </td>
          </tr>


          <tr onmouseout="chamnet_stop()" onmouseover="chamnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/chamnet.png' width="160"></div>
                  <img src='images/chamnet.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('hlstm_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('hlstm_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1812.08934">
                <papertitle>ChamNet: Towards Efficient Network Design through Platform-Aware Model Adaptation</papertitle>
              </a>
              <br>
              Xiaoliang Dai, Peizhao Zhang, Bichen Wu, <strong>Hongxu Yin</strong>, Fei Sun, Yanghan Wang, Marat Dukhan, Yunqing Hu, Yiming Wu, Yangqing Jia, Peter Vajda, Matt Uyttendaele, Niraj K. Jha
              <br>
               <em>CVPR</em>, 2019
              <br>
              <a href="https://github.com/facebookresearch/mobile-vision">code</a>
              /
              <a href="https://arxiv.org/abs/1812.08934">arXiv</a>
              <p></p>
              <p>
              A genetic algorithm to quickly adjust the hyper architecture of a base model to target platforms (DSP, CPU, GPU) given constraints such as latency or memory.
              </p>
            </td>
          </tr>



          <tr onmouseout="diabdeep_stop()" onmouseover="diabdeep_start()"  >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/diabdeep.png' width="160"></div>
                  <img src='images/diabdeep.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('diabedeep_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('diabedeep_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.04925">
                <papertitle>DiabDeep: Pervasive Diabetes Diagnosis based on Wearable Medical Sensors and Efficient Neural Networks</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Bilal Mukadam, Xiaoliang Dai, Niraj K. Jha
              <br>
               <em>IEEE Trans. Emerging Topics in Computing</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1910.04925">arXiv</a>
              <p></p>
              <p>
              Wearable medical sensors, backed by extremely efficient NNs, offers around-the-clock diabetes diagnosis.
              </p>
            </td>
          </tr>



          <tr onmouseout="hlstm_stop()" onmouseover="hlstm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/hlstm.png' width="160"></div>
                  <img src='images/hlstm.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('hlstm_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('hlstm_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1805.11797">
                <papertitle>Grow and Prune Compact, Fast, and Accurate LSTMs</papertitle>
              </a>
              <br>
              Xiaoliang Dai*,
              <strong>Hongxu Yin*</strong>, Niraj K Jha
              <br>
               <em>IEEE Trans. Computers</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1805.11797">arXiv</a>
              <p></p>
              <p>
              We show that grow-and-prune yields faster yet more accurate H-LSTM family, surpassing LSTMs and GRUs.
              </p>
            </td>
          </tr>


          <tr onmouseout="hardhlstm_stop()" onmouseover="hardhlstm_start()"  >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/hardhlstm.png' width="160"></div>
                  <img src='images/hardhlstm.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('di_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('di_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9556553">
                <papertitle>Towards Execution-Efficient LSTMs via Hardware-Guided Grow-and-Prune Paradigm</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Guoyang Chen, Yingmin Li, Shuai Che, Weifeng Zhang, Niraj K Jha
              <br>
               <em>IEEE Trans. Emerging Topics in Computing</em>, 2019
              <br>
              <a href="https://ieeexplore.ieee.org/document/9556553">arXiv</a>
              <p></p>
              <p>
              We observe high degree of non-monoticity in latency surface given shrinking model dimensions, and propose a systematic structural Grow-and-Prune way to exlpoit this for faster inference.
              </p>
            </td>
          </tr>



          <tr onmouseout="nest_stop()" onmouseover="nest_start()"   bgcolor="e8f7f8" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/nest.png' width="160"></div>
                  <img src='images/nest.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('hlstm_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('hlstm_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1711.02017">
                <papertitle>NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm</papertitle>
              </a>
              <br>
              Xiaoliang Dai,
              <strong>Hongxu Yin</strong>, Niraj K Jha
              <br>
               <em>IEEE Trans. Computers</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1711.02017">arXiv</a>
              <p></p>
              <p>
              Human brains grow before age 2 before pruning neurons for efficient synapsis afterwards. We propose grow-and-prune accordingly, and show consistent improvements over conventional pruning that always start from full models.
              </p>
            </td>
          </tr>





          <tr onmouseout="smarthealthcare_stop()" onmouseover="smarthealthcare_start()  ">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/smarthealthcare.png' width="160"></div>
                  <img src='images/smarthealthcare.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('smarthealthcare_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('smarthealthcare_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.amazon.com/Healthcare-Foundations-Trends-Electronic-Automation/dp/1680834401">
                <papertitle>Smart Healthcare</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Ayten Ozge Akmandor, Niraj K. Jha
              <br>
               <em>Foundations & Trends</em>, 2017, &nbsp <font color="red"><strong>(Book chapter)</strong></font>
              <br>
              <a href="https://www.amazon.com/Healthcare-Foundations-Trends-Electronic-Automation/dp/1680834401">arXiv</a>
              <p></p>
              <p>
              We aim to lay out foundations for pervasive healthcare from a wearables angle. Book now available at <a href="https://www.amazon.com/Healthcare-Foundations-Trends-Electronic-Automation/dp/1680834401">Amazon</a>.
            </p>
            </td>
          </tr>



          <tr onmouseout="fpga_stop()" onmouseover="fpga_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='di_image'>
                  <img src='images/fpga.png' width="160"></div>
                  <img src='images/fpga.png' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('fpga_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('fpga_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/7168793">
                <papertitle>Novel Real-time System Design for Floating-point sub-Nyquist Multi-coset Signal Blind reconstruction</papertitle>
              </a>
              <br>
              <strong>Hongxu Yin</strong>,
              Bah Hwee Gwee, Zhiping Lin, Anil Kumar, Sirajudeen Gulam Razul, Chong Meng Samson See
              <br>
               <em>ISCAS</em>, 2015 &nbsp <font color="red"><strong>(Oral presentation)</strong></font>
              <br>
              <a href="https://ieeexplore.ieee.org/document/7168793">arXiv</a>
              <p></p>
              <p>
              An FPGA solution augmented by a novel real-time tri-core SVD design for multi-coset signal reconstruction.
            </p>
            </td>
          </tr>


        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Patents</heading>
              <p>Partial list <a href="data/hongxu_cv.pdf">here</a>. </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Invited Talks</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
              &#9826; Data-efficient Deep Learning, <em>Keynote, ICLRW'23</em>
              <br>
              &#9826; GPU-based Efficient Deep Learning and Research Fronts, <em>Keynote, CVPRW'23</em>
              <br>
              &#9826; Towards Efficient and Secure Deep Learning, <em>Invited Keynote, Design & Automation Conference (DAC'60)</em>
              <br>
              &#9826; Towards Efficient and Secure Deep Nets, <em>University of British Columbia ECE Department</em>
              <br>
              &#9826; Inverting Deep Nets, <em>Princeton University, Department of Computer Science research groups</em>
              <br>
              &#9826; See through Gradients, <em>Europe ML meeting</em>
              <br>
              &#9826; Dreaming to Distill, <em>Synced AI (Êú∫Âô®‰πãÂøÉ)</em>
              <br>
              &#9826; Dreaming to Distill, <em>Facebook AR/VR</em>
              <br>
              &#9826; Making Neural Networks Efficient, <em>Alibaba Cloud / Platform AI group</em>
              <br>
              &#9826; Efficient Neural Networks, <em>Efficient Neural Networks</em>
              <br>
              &#9826; Efficient Neural Networks, <em>Baidu Research, ByteDance A.I. Lab US</em>
              <br>
              &#9826; Efficient Neural Networks, <em>Alibaba A.I. Research, Kwai Lab</em>
              <br>
              &#9826; Applied Machine Learning: From Theory to Practice, <em>Invited Keynote, IEEE Circuits and Systems Society (Singapore Chapter)</em>
              <br>
              &#9826; A Health Decision Support System for Disease Diagnosis, <em> New Jersey Tech Council</em>
              </td>
          </tr>

        </tbody></table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Reviewer & Committee</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
              <strong><em> (Conferences) </em></strong>
              <br>
              <br>
              &#9826; Computer Vision and Pattern Recognition (CVPR)
              <br>
              &#9826; Conference on Neural Information Processing Systems (NeurIPS)
              <br>
              &#9826; International Conference on Machine Learning (ICML)
              <br>
              &#9826; International Conference on Learning Representations (ICLR)
              <br>
              &#9826; European Conference on Computer Vision (ECCV)
              <br>
              &#9826; International Conference on Computer Vision (ICCV)
              <br>
              &#9826; British Machine Vision Conference (BMVC)
              <br>
              &#9826; Winter Conference on Applications of Computer Vision (WACV)
              <br>
              &#9826; AAAI Conference on Artificial Intelligence (AAAI)
              <br>
              &#9826; Design Automation Conference (DAC)
              <br>
              &#9826; High-Performance Computer Architecture (HPCA)
              <br>
              <br>
              <br>
              <strong><em> (Journals) </em></strong>
              <br>
              <br>
              &#9826; IEEE Transactions on Pattern Analysis and Machine Intelligence
              <br>
              &#9826; IEEE Transactions on Neural Networks and Learning Systems
              <br>
              &#9826; International Journal of Computer Vision
              <br>
              &#9826; IEEE Journal of Biomedical and Health Informatics
              <br>
              &#9826; IEEE Journal of Selected Topics in Signal Processing
              <br>
              &#9826; IEEE Sensors Journal
              <br>
              &#9826; IEEE Consumer Electronics Magazine
              <br>
              &#9826; International Journal on Artificial Intelligence Tools
              <br>
              &#9826; International Journal of Systems Architecture
              <br>
              &#9826; International Journal of Healthcare Technology and Management
              <br>
              &#9826; International Journal of Electronic Imaging
              <br>

              </td>
          </tr>

        </tbody></table>




        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Mentorship</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">

              <strong><em> (NVIDIA Research Interns) </em></strong>
              <br>
              <br>
              &#9826; Baifeng Shi, Ph.D., University of California, Berkeley
              <br>
              &#9826; Hanrong Ye, Ph.D., Hong Kong University of Science and Technology
              <br>
              &#9826; Ji Lin, Ph.D., Massachusetts Institute of Technology
              <br>
              <!-- &#9826; Zhuolin Yang, Ph.D., University of Illinois Urbana-Champaign
              <br> -->
              &#9826; Zhen Dong, Ph.D., University of California, Berkeley
              <br>
              &#9826; Huanrui Yang, Ph.D., Duke University
              <br>
              &#9826; Xin Dong, Ph.D., Harvard University
              <br>
              &#9826; Divyam Madaan, Ph.D., New York University
              <br>
              <!-- &#9826; Shixing Yu, Ph.D., University of Texas, Austin
              <br> -->
              &#9826; Annamarie Bair, Ph.D., Carnegie Mellon University
              <!-- <br>
              &#9826; Alex Sun, B.E., University of Illinois Urbana-Champaign
              <br>
              &#9826; Paul Micaelli, Ph.D., University of Edingbugh
              <br>
              &#9826; Yerlan Idelbayev, Ph.D., University of California, Merced
              <br>
              &#9826; Vu Nguyen, Ph.D., Stony Brooks University
              <br>
              &#9826; Akshay Chawla, M.E., Carnegie Mellon University -->
              <br>
              <br>
              <br>


              <strong><em> (Princeton Senior Thesis Mentees) </em></strong>
              <br>
              <br>
              &#9826; Joe Zhang, now Ph.D. at Stanford
              <br>
              &#9826; Hari Santhanam, now Ph.D. at University of Pennsylvania
              <br>
              &#9826; Frederick Hertan, now at SIG Trading
              <br>
              &#9826; Kyle Johnson, now at Princeton University
              <br>
              &#9826; Bilal Mukadam, now at Microsoft
              <br>
              &#9826; Chloe Song, now at Astra Inc.
              <br>
              <br>
              <br>

              </td>
          </tr>

        </tbody></table>


        <table width="100%" align="right" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr align="right">
            <td>(web template from <a href="https://jonbarron.info/">here</a>, with thanks)</td>
          </tr>

        </tbody></table>


      </td>
    </tr>





  </table>
</body>

</html>
